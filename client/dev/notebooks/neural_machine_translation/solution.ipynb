{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f97dcbb-b950-4711-a1c8-b6fe1a7e48a2",
   "metadata": {
    "contenteditable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8830ed5b-48e8-4911-9595-c4df30f778fc",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "editable": false,
    "gai_cell_type": "SETUP",
    "gai_description": "This cell will load the data and defines a function to build and train the neural network.",
    "gai_title": "Setup",
    "hidden": true,
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess(x, x_tokenizer, y, y_tokenizer):\n",
    "    raw_x = tokenize_and_pad(x_tokenizer, x)\n",
    "    raw_y = tokenize_and_pad(y_tokenizer, y)\n",
    "    print(\"X Raw Shape: %s\"%(raw_x.shape,))\n",
    "    print(\"Y Raw Shape: %s\"%(raw_y.shape,))\n",
    "    raw_x = pad_sequences(raw_x, padding='post', maxlen=raw_y.shape[1])\n",
    "    return raw_x.reshape(*raw_x.shape, 1), raw_y.reshape(*raw_y.shape, 1)\n",
    "    \n",
    "def tokenize_and_pad(tokenizer, x):\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(x), padding='post')\n",
    "    \n",
    "def output_to_text(output_logits, tokenizer):\n",
    "    index_to_words = create_word_lookup_dict(tokenizer.word_index)     \n",
    "    return \" \".join([index_to_words[prediction] for prediction in np.argmax(output_logits, 1)])\n",
    "\n",
    "def create_word_lookup_dict(word_index):\n",
    "    index_to_words = {id: word for word, id in word_index.items()}\n",
    "    index_to_words[0] = \"\"\n",
    "\n",
    "    return index_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8225ac0-5521-4ab8-87e8-74052b637e02",
   "metadata": {
    "deletable": false,
    "gai_cell_type": "MODEL",
    "gai_description": "This cell has the headers for the preprocessing and output postprocessing functions that you should write.",
    "gai_title": "Model"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# INPUT PROCESSING\n",
    "# x: list of strings (English sentences)\n",
    "# y: list of strings (French translations)\n",
    "#--------------------------------------------\n",
    "\n",
    "# STEP 1: Preprocessing Steps\n",
    "#    1a: Call function to tokenize and pad (define later in Step 2)\n",
    "#    1b: pad English data to have same sentence length as French\n",
    "#    1c: Adjust the dimensions to fit the Keras loss function\n",
    "def preprocess(x, x_tokenizer, y, y_tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess the sentences in x and y\n",
    "    :param x: list of sentence strings\n",
    "    :param x_tokenzier: tokenizer for x\n",
    "    :param y: list of sentence strings\n",
    "    :param: y_tokenizer: tokenizer for y\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y)\n",
    "    \"\"\"\n",
    "    # 1a: Tokenize and Pad x any y (Go to definition in Step 2)\n",
    "    raw_x = tokenize_and_pad(x_tokenizer, x)\n",
    "    raw_y = tokenize_and_pad(y_tokenizer, y)\n",
    "    \n",
    "    # 1b: pad English data to have same sentence length as French\n",
    "    print(\"X Raw Shape: %s\"%(raw_x.shape,))\n",
    "    print(\"Y Raw Shape: %s\"%(raw_y.shape,))\n",
    "    \n",
    "    # 1c: Adjust the dimensions to fit the Keras loss function\n",
    "    \n",
    "    raise Exception(\"preprocess function not complete\")\n",
    "\n",
    "    \n",
    "# Step 2: Tokenize and Pad Function\n",
    "#   2a: Initialize tokenizer\n",
    "#   2b: Convert sentences to lists of integer word identifiers\n",
    "#   2c: Pad sequences: convert data to 2D ndarray (sentence number, word number)\n",
    "#         by adding 0s to fill in blanks\n",
    "def tokenize_and_pad(tokenizer, x):\n",
    "    \"\"\"\n",
    "    Tokenize and pad x\n",
    "    :param tokenizer: Tokenizer object from Keras \n",
    "    :param x: list of sentences/strings to be tokenized and padded\n",
    "    :return: tokenized and padded x data with pads added to end\n",
    "    \"\"\"\n",
    "    # 2a: Initialize tokenizer with input texts\n",
    "    \n",
    "    # 2b: Convert to integers\n",
    "    \n",
    "    # 2c: Pad sequences\n",
    "    \n",
    "    raise Exception(\"tokenize_and_pad function not implemented\")\n",
    "    \n",
    "#--------------------------------------------\n",
    "# OUTPUT PROCESSING\n",
    "# Once we remove the extra dimension needed by the loss function,\n",
    "# the output shape is (word number in sentence, French vocabulary items)\n",
    "#--------------------------------------------\n",
    "\n",
    "# Step 3: Convert a single output from the NN into a sentence\n",
    "#   3a: Create lookup table for words from indices (define later)\n",
    "#   3b: Convert output into words\n",
    "def output_to_text(output_logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn output from neural network into text using the tokenizer\n",
    "    :param output: output from the neural network with the extra dimension removed\n",
    "    :param tokenizer: Keras Tokenizer for the output language\n",
    "    :return: String that represents the text of the output\n",
    "    \"\"\"\n",
    "    # 3a:Create lookup table for words (Go to Function Definition below)\n",
    "    # Returns: {int index : str 'word' }\n",
    "    index_to_words = create_word_lookup_dict(tokenizer.word_index)     \n",
    "\n",
    "    # 3b: Convert output to words\n",
    "    raise Exception(\"output_to_text function not implemented\")\n",
    "\n",
    "# Step 3a: Create lookup table for words from indices (define later) \n",
    "def create_word_lookup_dict(word_index):\n",
    "    \"\"\"\n",
    "    :param word_index: Tokenizer dictionary mapping words to integer identifiers {'str' : int}\n",
    "    :return dictionary mapping integer to words or '<PAD>' {int : 'str'}\n",
    "    \"\"\"\n",
    "    raise Exception(\"create_word_lookup_dict function not implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6e9e8-6717-498c-8f85-c31c129f6b02",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "VALIDATION",
    "gai_description": "This cell prints an English sentence, its correct French translation, and the translation from the completed network.",
    "gai_title": "Validation",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "# VALIDATION:\n",
    "preproc_english_sentences, preproc_french_sentences = preprocess(english_sentences,\n",
    "                                                            english_tokenizer,\n",
    "                                                            french_sentences,\n",
    "                                                            french_tokenizer)\n",
    "# load the pre-trainined network\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "model = load_model(f\"{tempfile.gettempdir()}/translation.tf\")\n",
    "# TEST: This translates a single sentence from English to French\n",
    "pred = model.predict(preproc_english_sentences[:1], verbose=0)       # Predict the French sentence from NN\n",
    "# OUTPUT SHAPE: (extra dimension of size 1, word number in sentence, French vocabulary items)\n",
    "pred = pred[0]                                                       # Remove Keras-required 3rd dimension\n",
    "translation = output_to_text(pred, french_tokenizer)                 # To human readable French sentence\n",
    "output = \"English\\n\"\n",
    "output += \" * preproc_english_sentences_type: %s\\n\"%(str(type(preproc_english_sentences)))\n",
    "output += \" * preproc_english_sentences_shape: %s\\n\"%(str(preproc_english_sentences.shape))\n",
    "output += \"French\\n\"\n",
    "output += \" * preproc_french_sentences_type: %s\\n\"%(str(type(preproc_french_sentences)))\n",
    "output += \" * preproc_french_sentences_shape: %s\\n\"%(str(preproc_french_sentences.shape))\n",
    "output += \"Test Case - English to French\\n\"\n",
    "output += \" * English sentence: %s\\n\"%(english_sentences[0])\n",
    "output += \" * Actual translation: %s\\n\"%(french_sentences[0])\n",
    "output += \" * Predicted translation: %s\\n\"%(translation)\n",
    "result.append(json.dumps(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
