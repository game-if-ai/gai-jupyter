{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8830ed5b-48e8-4911-9595-c4df30f778fc",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "SETUP",
    "gai_description": "This is the setup cell. It will generate the testing and training data for your model.",
    "gai_title": "Setup",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    # Load input file\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split(\"\\n\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, TimeDistributed, Dropout\n",
    "from keras.models import load_model\n",
    "from IPython.display import JSON\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# Build the RNN layers\n",
    "def simple_model(input_shape, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size + 1, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load English data\n",
    "english_sentences = load_data('neural_machine_translation/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('neural_machine_translation/small_vocab_fr')\n",
    "\n",
    "french_tokenizer = Tokenizer()\n",
    "english_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8225ac0-5521-4ab8-87e8-74052b637e02",
   "metadata": {
    "deletable": false,
    "gai_cell_type": "MODEL",
    "gai_description": "This is the model cell. It has been filled in with stub code that you should edit.",
    "gai_title": "Model"
   },
   "outputs": [],
   "source": [
    "def preprocess_func(x, x_tokenizer, y, y_tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param x_tokenzier: tokenizer for x\n",
    "    :param y: Label List of sentences\n",
    "    :param: y_tokenizer: tokenizer for y\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y)\n",
    "    \"\"\"\n",
    "    # TODO: add code here\n",
    "    # Note: Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    raise Exception(\"Function not implemented\")\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    # TODO: add code here\n",
    "    raise Exception(\"Function not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dea6e9e8-6717-498c-8f85-c31c129f6b02",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "VALIDATION",
    "gai_description": "This is the validation cell. It will use the testing data to generate outputs from your model.",
    "gai_title": "Validation",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "Actual translation: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 21:43:00.846619: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-03 21:43:00.849104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-03 21:43:00.850237: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-03 21:43:00.990129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-03 21:43:00.991351: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-03 21:43:00.992497: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: new jersey est parfois chaud en mois et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "preproc_english_sentences_shape (137861, 21, 1)\n",
      "preproc_english_sentences_type <class 'numpy.ndarray'>\n",
      "preproc_french_sentences_shape (137861, 21, 1)\n",
      "preproc_french_sentences_type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "preproc_english_sentences, preproc_french_sentences = preprocess_func(english_sentences,\n",
    "                                                            english_tokenizer,\n",
    "                                                            french_sentences,\n",
    "                                                            french_tokenizer)\n",
    "# Train the neural network\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "model = load_model(\"neural_machine_translation/translation.tf\")\n",
    "\n",
    "print(\"English sentence:\", english_sentences[0])\n",
    "print(\"Actual translation:\", french_sentences[0])\n",
    "print(\"Predicted translation:\", logits_to_text(model.predict(preproc_english_sentences[:1], verbose=0)[0], french_tokenizer))\n",
    "print(\"preproc_english_sentences_shape\", str(preproc_english_sentences.shape))\n",
    "print(\"preproc_english_sentences_type\", str(type(preproc_english_sentences)))\n",
    "print(\"preproc_french_sentences_shape\", str(preproc_french_sentences.shape))\n",
    "print(\"preproc_french_sentences_type\", str(type(preproc_french_sentences)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
