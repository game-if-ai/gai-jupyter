{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f97dcbb-b950-4711-a1c8-b6fe1a7e48a2",
   "metadata": {
    "contenteditable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8830ed5b-48e8-4911-9595-c4df30f778fc",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "editable": false,
    "gai_cell_type": "SETUP",
    "gai_description": "This cell will load the data and defines a function to build and train the neural network.",
    "gai_title": "Setup",
    "hidden": true,
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    # Load input file\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split(\"\\n\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, TimeDistributed, Dropout\n",
    "from keras.models import load_model\n",
    "import tempfile\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# Build the RNN layers\n",
    "def simple_model(input_shape, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size + 1, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load English data\n",
    "english_sentences = load_data(f'{tempfile.gettempdir()}/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data(f'{tempfile.gettempdir()}/small_vocab_fr')\n",
    "\n",
    "french_tokenizer = Tokenizer()\n",
    "english_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8225ac0-5521-4ab8-87e8-74052b637e02",
   "metadata": {
    "deletable": false,
    "gai_cell_type": "MODEL",
    "gai_description": "This cell has the headers for the preprocessing and output postprocessing functions that you should write.",
    "gai_title": "Model"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# INPUT PROCESSING\n",
    "# x: list of strings (English sentences)\n",
    "# y: list of strings (French translations)\n",
    "#--------------------------------------------\n",
    "\n",
    "# STEP 1: Preprocessing Steps\n",
    "#    1a: Call function to tokenize and pad (define later in Step 2)\n",
    "#    1b: pad English data to have same sentence length as French\n",
    "#    1c: Adjust the dimensions to fit the Keras loss function\n",
    "def preprocess(x, x_tokenizer, y, y_tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess the sentences in x and y\n",
    "    :param x: list of sentence strings\n",
    "    :param x_tokenzier: tokenizer for x\n",
    "    :param y: list of sentence strings\n",
    "    :param: y_tokenizer: tokenizer for y\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y)\n",
    "    \"\"\"\n",
    "    # 1a: Tokenize and Pad x any y (Go to definition in Step 2)\n",
    "    raw_x = tokenize_and_pad(x_tokenizer, x)\n",
    "    raw_y = tokenize_and_pad(y_tokenizer, y)\n",
    "    \n",
    "    # 1b: pad English data to have same sentence length as French\n",
    "    print(\"X Raw Shape: %s\"%(raw_x.shape,))\n",
    "    print(\"Y Raw Shape: %s\"%(raw_y.shape,))\n",
    "    \n",
    "    # 1c: Adjust the dimensions to fit the Keras loss function\n",
    "    \n",
    "    raise Exception(\"preprocess function not complete\")\n",
    "\n",
    "    \n",
    "# Step 2: Tokenize and Pad Function\n",
    "#   2a: Initialize tokenizer\n",
    "#   2b: Convert sentences to lists of integer word identifiers\n",
    "#   2c: Pad sequences: convert data to 2D ndarray (sentence number, word number)\n",
    "#         by adding 0s to fill in blanks\n",
    "def tokenize_and_pad(tokenizer, x):\n",
    "    \"\"\"\n",
    "    Tokenize and pad x\n",
    "    :param tokenizer: Tokenizer object from Keras \n",
    "    :param x: list of sentences/strings to be tokenized and padded\n",
    "    :return: tokenized and padded x data with pads added to end\n",
    "    \"\"\"\n",
    "    # 2a: Initialize tokenizer with input texts\n",
    "    \n",
    "    # 2b: Convert to integers\n",
    "    \n",
    "    # 2c: Pad sequences\n",
    "    \n",
    "    raise Exception(\"tokenize_and_pad function not implemented\")\n",
    "    \n",
    "#--------------------------------------------\n",
    "# OUTPUT PROCESSING\n",
    "# Once we remove the extra dimension needed by the loss function,\n",
    "# the output shape is (word number in sentence, French vocabulary items)\n",
    "#--------------------------------------------\n",
    "\n",
    "# Step 3: Convert a single output from the NN into a sentence\n",
    "#   3a: Create lookup table for words from indices (define later)\n",
    "#   3b: Convert output into words\n",
    "def output_to_text(output_logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn output from neural network into text using the tokenizer\n",
    "    :param output: output from the neural network with the extra dimension removed\n",
    "    :param tokenizer: Keras Tokenizer for the output language\n",
    "    :return: String that represents the text of the output\n",
    "    \"\"\"\n",
    "    # 3a:Create lookup table for words (Go to Function Definition below)\n",
    "    # Returns: {int index : str 'word' }\n",
    "    index_to_words = create_word_lookup_dict(tokenizer.word_index)     \n",
    "\n",
    "    # 3b: Convert output to words\n",
    "    raise Exception(\"output_to_text function not implemented\")\n",
    "\n",
    "# Step 3a: Create lookup table for words from indices (define later) \n",
    "def create_word_lookup_dict(word_index):\n",
    "    \"\"\"\n",
    "    :param word_index: Tokenizer dictionary mapping words to integer identifiers {'str' : int}\n",
    "    :return dictionary mapping integer to words or '<PAD>' {int : 'str'}\n",
    "    \"\"\"\n",
    "    raise Exception(\"create_word_lookup_dict function not implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6e9e8-6717-498c-8f85-c31c129f6b02",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "VALIDATION",
    "gai_description": "This cell prints an English sentence, its correct French translation, and the translation from the completed network.",
    "gai_title": "Validation",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "# VALIDATION:\n",
    "preproc_english_sentences, preproc_french_sentences = preprocess(english_sentences,\n",
    "                                                            english_tokenizer,\n",
    "                                                            french_sentences,\n",
    "                                                            french_tokenizer)\n",
    "# VERIFY: Look at types and shapes of the preprocessed Data\n",
    "print(\"English\")\n",
    "print(\" * preproc_english_sentences_type\", str(type(preproc_english_sentences)))\n",
    "print(\" * preproc_english_sentences_shape\", str(preproc_english_sentences.shape))\n",
    "print(\"French\")\n",
    "print(\" * preproc_french_sentences_type\", str(type(preproc_french_sentences)))\n",
    "print(\" * preproc_french_sentences_shape\", str(preproc_french_sentences.shape))\n",
    "# load the pre-trainined network\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "model = load_model(f\"{tempfile.gettempdir()}/translation.tf\")\n",
    "# TEST: This translates a single sentence from English to French\n",
    "pred = model.predict(preproc_english_sentences[:1], verbose=0)       # Predict the French sentence from NN\n",
    "# OUTPUT SHAPE: (extra dimension of size 1, word number in sentence, French vocabulary items)\n",
    "pred = pred[0]                                                       # Remove Keras-required 3rd dimension\n",
    "translation = output_to_text(pred, french_tokenizer)                 # To human readable French sentence\n",
    "print(\"Test Case - English to French\")\n",
    "print(\" * English sentence:\", english_sentences[0])\n",
    "print(\" * Actual translation:\", french_sentences[0])\n",
    "print(\" * Predicted translation:\", translation)\n",
    "result = json.dumps(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
