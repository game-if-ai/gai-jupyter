{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830ed5b-48e8-4911-9595-c4df30f778fc",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "SETUP",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    # Load input file\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split(\"\\n\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, TimeDistributed, Dropout\n",
    "\n",
    "import numpy as np\n",
    "# Build the RNN layers\n",
    "def simple_model(input_shape, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size + 1, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load English data\n",
    "english_sentences = load_data('neural_network/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('neural_network/small_vocab_fr')\n",
    "\n",
    "french_tokenizer = Tokenizer()\n",
    "english_tokenizer = Tokenizer()\n",
    "\n",
    "def preprocess_data_and_fit_model(preprocess_func):\n",
    "    preproc_english_sentences, preproc_french_sentences = preprocess_func(english_sentences,\n",
    "                                                                 english_tokenizer,\n",
    "                                                                 french_sentences,\n",
    "                                                                 french_tokenizer)\n",
    "    french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "    # Train the neural network\n",
    "    model = simple_model(\n",
    "        preproc_english_sentences.shape,\n",
    "        french_vocab_size)\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(preproc_english_sentences, preproc_french_sentences, batch_size=300,\n",
    "                         epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8225ac0-5521-4ab8-87e8-74052b637e02",
   "metadata": {
    "deletable": false,
    "gai_cell_type": "MODEL"
   },
   "outputs": [],
   "source": [
    "# preprocess sentences in x and y and\n",
    "# return tuple of x and y after tokenization, padding and reshaping\n",
    "def preprocess_func(x, x_tokenizer, y, y_tokenizer):\n",
    "    # TODO: add code here\n",
    "    return\n",
    "\n",
    "preprocess_data_and_fit_model(preprocess_func) # Do not change this line\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    # TODO: add code here\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6e9e8-6717-498c-8f85-c31c129f6b02",
   "metadata": {
    "contenteditable": false,
    "deletable": false,
    "gai_cell_type": "VALIDATION",
    "tags": [
     "highlight"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"English sentence:\", english_sentences[0])\n",
    "print(\"Actual translation:\", french_sentences[0])\n",
    "print(\"Predicted translation:\", logits_to_text(model.predict(preproc_english_sentences[:1])[0], french_tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
